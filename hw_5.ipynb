{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tabulate as tab\n",
    "\n",
    "columns = 20\n",
    "lines = 20\n",
    "total_states = columns*lines\n",
    "\n",
    "pU = np.zeros((total_states,total_states))\n",
    "pD = np.zeros((total_states,total_states))\n",
    "pL = np.zeros((total_states,total_states))\n",
    "pR = np.zeros((total_states,total_states))\n",
    "\n",
    "greylight_areas = [47,48,49,50,60,61,62,63,64,65,66,67,70,80,90,100,110,120,121,122,123,124,125,126,127,130,147,150,167,170,187,190,207,210,227,230,247,250,267,268,269,270]\n",
    "greydark_areas = [81,82,83,84,85,86,87,88,89,101,102,103,104,105,106,107,108,109,128,129,148,149,168,169,188,189,208,109,228,229,240,248,249]\n",
    "goal = [18,19,39]\n",
    "\n",
    "actions_name = {0: 'U', 1: 'D', 2: 'L', 3: 'R'}\n",
    "actions_matrix = {'U': pU, 'D': pD, 'L': pL, 'R': pR}\n",
    "\n",
    "cost = np.full((total_states, len(actions_name)), 0.05)\n",
    "\n",
    "def calcCost():\n",
    "    for pos in greylight_areas:\n",
    "        cost[pos] = 0.5\n",
    "    \n",
    "    for pos in greydark_areas:\n",
    "        cost[pos] = 1\n",
    "    \n",
    "    for pos in goal:\n",
    "        cost[pos] = 0\n",
    "\n",
    "def convertedPos(x,y):\n",
    "    return x*columns + y\n",
    "\n",
    "def getAdjacents(x,y):\n",
    "    valid_adjacents = dict()\n",
    "    adjacents = [(x-1,y),(x+1,y),(x,y-1),(x,y+1)]\n",
    "    \n",
    "    for index, adjacent in enumerate(adjacents):\n",
    "        action_name = actions_name[index]\n",
    "        \n",
    "        if 0 <= adjacent[0] < columns and 0 <= adjacent[1] < columns:\n",
    "            valid_adjacents[action_name] = convertedPos(adjacent[0],adjacent[1])   \n",
    "        else:\n",
    "            valid_adjacents[action_name] = None\n",
    "\n",
    "    return valid_adjacents\n",
    "\n",
    "for x in range(columns):\n",
    "    for y in range(lines):\n",
    "        n_state = convertedPos(x,y)\n",
    "        adjacents = getAdjacents(x,y)\n",
    "               \n",
    "        for action in actions_matrix:\n",
    "            current_matrix = actions_matrix[action]\n",
    "            current_matrix[n_state,n_state] += 0.02\n",
    "            \n",
    "            for adjacent in adjacents:\n",
    "                adjacent_state = adjacents[adjacent]\n",
    "            \n",
    "                if adjacent == action and adjacents[action] != None:\n",
    "                    current_matrix[n_state,adjacent_state] += 0.92\n",
    "                    continue\n",
    "                elif adjacent == action:\n",
    "                    current_matrix[n_state,n_state] += 0.92\n",
    "                    continue\n",
    "                \n",
    "                if adjacents[adjacent] != None:\n",
    "                    current_matrix[n_state,adjacent_state] += 0.02\n",
    "                    continue\n",
    "                else:\n",
    "                    current_matrix[n_state,n_state] += 0.02\n",
    "                    continue\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25    0.32055 0.32    0.25   ]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "total_actions = len(actions_matrix)\n",
    "Q = np.zeros((total_states, total_actions))\n",
    "Q[convertedPos(3,16)] = np.array([0.25,0.32,0.32,0.25])\n",
    "Q[convertedPos(4,16)] = np.array([0.29,0.36,0.36,0.29])\n",
    "\n",
    "def getMin(state_values):  \n",
    "    min_value = np.amin(state_values)  \n",
    "    min_indexes = np.where(state_values == min_value)\n",
    "\n",
    "    return np.random.choice(min_indexes[0])\n",
    "             \n",
    "def minAction(Qfunc, state, epsilon = 0.1):\n",
    "    next_action = getMin(Qfunc[state])\n",
    "    \n",
    "    choice = np.random.choice(['exploit', 'explore'], 1, p = [1 - epsilon, epsilon])\n",
    "    \n",
    "    if choice == 'exploit':\n",
    "        return next_action\n",
    "    elif choice == 'explore':\n",
    "        return np.random.randint(len(actions_name.keys()))\n",
    "          \n",
    "def qvals_Qlearning(state, next_action, Q, next_state):\n",
    "    #next_action = minAction(Q, state)\n",
    "    next_action_name = actions_name[next_action]\n",
    "    next_action_matrix = actions_matrix[next_action_name]\n",
    "    #next_state = np.argmax(next_action_matrix[state])\n",
    "    \n",
    "    Q[state][next_action] += alpha * (cost[state][next_action] + gamma * np.amin(Q[next_state]) - Q[state][next_action])\n",
    "\n",
    "    \n",
    "state = convertedPos(3,16)\n",
    "next_state = convertedPos(4,16)\n",
    "qvals_Qlearning(state, 1, Q, next_state)\n",
    "    \n",
    "print(Q[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25   0.3272 0.32   0.25  ]\n"
     ]
    }
   ],
   "source": [
    "Qsarsa= np.zeros((total_states, total_actions))\n",
    "Qsarsa[convertedPos(3,16)] = np.array([0.25,0.32,0.32,0.25])\n",
    "Qsarsa[convertedPos(4,16)] = np.array([0.29,0.36,0.36,0.29])\n",
    "\n",
    "def qvals_sarsa(state, next_action, Q, next_state):\n",
    "    #next_action = minAction(Q, state)\n",
    "    next_action_name = actions_name[next_action]\n",
    "    next_action_matrix = actions_matrix[next_action_name]\n",
    "    #next_state = np.argmax(next_action_matrix[state])\n",
    "    \n",
    "    Q[state][next_action] += alpha * (cost[state][next_action] + gamma * Q[next_state][next_action] - Q[state][next_action])\n",
    "\n",
    "    \n",
    "state = convertedPos(3,16)\n",
    "next_state = convertedPos(4,16)\n",
    "qvals_sarsa(state, 1, Qsarsa,  next_state)\n",
    "\n",
    "print(Qsarsa[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An off-policy learner learns the value of the optimal policy independently of the agent's actions. Q-learning is an off-policy learner. An on-policy learner learns the value of the policy being carried out by the agent including the exploration steps.\"\n",
    "\n",
    "The reason that Q-learning is off-policy is that it updates its Q-values using the Q-value of the next state s′ and the greedy action a′. In other words, it estimates the return (total discounted future reward) for state-action pairs assuming a greedy policy were followed despite the fact that it's not following a greedy policy.\n",
    "\n",
    "The reason that SARSA is on-policy is that it updates its Q-values using the Q-value of the next state s′ and the current policy's action a′′. It estimates the return for state-action pairs assuming the current policy continues to be followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
